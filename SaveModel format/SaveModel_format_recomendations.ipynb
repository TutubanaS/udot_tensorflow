{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "oBrv5DyjtUAP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "def restart_runtime():\n",
        "  os.kill(os.getpid(), 9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "hDBAIO9lefQw"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "%%bash\n",
        "pip install tensorflow_hub\n",
        "pip install tensorflow_text\n",
        "pip install tensorflow\n",
        "pip install tensorflow_recommenders\n",
        "pip install sklearn\n",
        "pip install 'scikit_learn~=0.23.0' #for random_gaussian projection matrix\n",
        "pip install apache-beam[interactive] #tensorflow pipeline library\n",
        "pip install scann #index library\n",
        "pip install python-snappy #to speed up TFrecord managment"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/UniversalDot/tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0n6hxMgE8fl",
        "outputId": "91e97f0f-c7cd-4684-e648-a59d9ccc4969"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'tensorflow'...\n",
            "remote: Enumerating objects: 38, done.\u001b[K\n",
            "remote: Counting objects: 100% (23/23), done.\u001b[K\n",
            "remote: Compressing objects: 100% (18/18), done.\u001b[K\n",
            "remote: Total 38 (delta 5), reused 21 (delta 4), pack-reused 15\u001b[K\n",
            "Unpacking objects: 100% (38/38), done.\n",
            "Checking out files: 100% (11/11), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x1osAAvutIDi"
      },
      "outputs": [],
      "source": [
        "restart_runtime()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vWkaHNwwYKLP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73867a5e-8500-4095-b48d-5e19e00c312f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-09-27 07:51:01.054790: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2022-09-27 07:51:01.224902: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2022-09-27 07:51:01.977849: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2022-09-27 07:51:01.978016: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2022-09-27 07:51:01.978027: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import os\n",
        "\n",
        "import tensorflow as tf\n",
        "import sklearn\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVYujeDujZFn"
      },
      "source": [
        "### Content based model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LullnqQt7Ojq"
      },
      "source": [
        "Creating a tensorflow model for indexing\n",
        "\n",
        "\n",
        "0.   Create embeddings and matrix projection preprocess\n",
        "1.   Function to add and remove elements in the index\n",
        "2.   Use ScanNN to index the closest embed in the file\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "TYz0n-oIDkpq"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf \n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text\n",
        "import tensorflow_recommenders as tfrc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "QulINrP3IQLC"
      },
      "outputs": [],
      "source": [
        "def load_embedding(url = 'https://tfhub.dev/google/universal-sentence-encoder-multilingual/3'):\n",
        "  return hub.load(url)\n",
        "\n",
        "def embed_text(input, model = None):\n",
        "  if model == None:\n",
        "    model = load_embedding()\n",
        "  return model.predict(input)\n",
        "\n",
        "def random_projection_matrix_gen(original_dim:int, projected_dim:int, save_pickle:bool = False):\n",
        "  \"\"\"\n",
        "  ## Inputs\n",
        "  - **original_dim**: dimension of the embedding space\n",
        "  - **project_dm**: dimension of the output shrinked space\n",
        "  - **save_pickle**: if True saves the matrix weights in a pickle file(default: False)\n",
        "  \n",
        "  ## Outputs\n",
        "  Random projection matrix as np array \n",
        "  \"\"\"\n",
        "  import pickle\n",
        "  from sklearn.random_projection import gaussian_random_matrix\n",
        "\n",
        "  random_projection_matrix = None\n",
        "  if projected_dim and original_dim > projected_dim:\n",
        "    random_projection_matrix = gaussian_random_matrix(\n",
        "        n_components=projected_dim, n_features=original_dim).T\n",
        "    print(\"A Gaussian random weight matrix was creates with shape of {}\".format(random_projection_matrix.shape))\n",
        "    \n",
        "    if save_pickle:\n",
        "      print('Storing random projection matrix to disk...')\n",
        "      with open('random_projection_matrix', 'wb') as handle:\n",
        "        pickle.dump(random_projection_matrix, \n",
        "                    handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "  return random_projection_matrix"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_embedding(input, embedding_model, random_project_matrix = None):\n",
        "  \n",
        "  if isinstance(input, tf.Tensor):\n",
        "    x = embedding_model.predict(input)['outputs'].numpy()\n",
        "  else:\n",
        "    x = embedding_model.predict(tf.constant(input))['outputs'].numpy()\n",
        "  \n",
        "  if random_project_matrix is not None:\n",
        "    x.dot(random_project_matrix)\n",
        "    \n",
        "  return x\n",
        "\n",
        "def embed_fun(array, batch_size, embedding_model, rpm):\n",
        "    for i in range(0, len(array), batch_size):\n",
        "      batch = array[i:i+batch_size]\n",
        "      x = embedding_model(batch).numpy()\n",
        "      if rpm:\n",
        "        x = x.dot(rpm)\n",
        "      yield batch, x\n"
      ],
      "metadata": {
        "id": "_4mRTfxUIt-b"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _format_jobdesc(string:str) -> str:\n",
        "  formatted_string = string.replace('\\xa0', ' ')\n",
        "  formatted_string = formatted_string.replace('-', '')\n",
        "  formatted_string = formatted_string.replace('â€¢', '')\n",
        "\n",
        "  return formatted_string"
      ],
      "metadata": {
        "id": "fpr5myHE4aHd"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_model = load_embedding()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1u9JQFj48om",
        "outputId": "1c7600d8-90d3-4460-ba04-befb190b32b5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-09-27 07:51:16.868789: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-09-27 07:51:17.063038: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-09-27 07:51:17.063882: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-09-27 07:51:17.065529: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2022-09-27 07:51:17.065861: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-09-27 07:51:17.066497: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-09-27 07:51:17.067103: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-09-27 07:51:18.148908: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-09-27 07:51:18.150050: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-09-27 07:51:18.150956: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-09-27 07:51:18.151797: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2022-09-27 07:51:18.151851: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15361 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "jobdesc = pd.read_csv('/content/tensorflow/dataset/job_desc.csv', index_col = None, names = None)\n",
        "jobdesc = np.array(list(jobdesc.jobdescription)) \n",
        "jobdesc = [_format_jobdesc(s) for s in jobdesc]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrUOjnwuFJtv",
        "outputId": "b8462cf5-8d16-4bd0-8aa7-2dd016596845"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "tcmalloc: large alloc 2145624064 bytes == 0x39fda000 @  0x7f61fe42a001 0x7f61f04aa1af 0x7f61f0506557 0x7f61f0506d1b 0x7f61f05a7333 0x58eb9c 0x51b4e6 0x5b41c5 0x604133 0x631049 0x58ea5d 0x51ae13 0x4cfb74 0x51b7ef 0x4cfb74 0x51b7ef 0x4cfb74 0x4d19df 0x51b31c 0x58f2a7 0x51740e 0x58f2a7 0x517947 0x5b41c5 0x4ba80a 0x4d29f9 0x5913c6 0x51908c 0x5b4a3e 0x58f49e 0x51837f\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "from tqdm import tqdm\n",
        "\n",
        "s = np.ndarray((0), dtype = str)\n",
        "emb = np.ndarray((0, 512), dtype = np.float32)\n",
        "for batch in tqdm(embed_fun(jobdesc, 128, embedding_model, None)):\n",
        "  s = np.append(s, batch[0])\n",
        "  emb = np.append(emb, batch[1], axis=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pTxC9Ibuq2hR",
        "outputId": "2159ce53-2444-4540-880b-471e64fcc3a2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "99it [00:37,  1.66it/s]tcmalloc: large alloc 1210269696 bytes == 0xc15d2000 @  0x7f61fe42a001 0x7f61f04aa1af 0x7f61f0500c23 0x7f61f05a986d 0x7f61f05aa17f 0x7f61f05aa2d0 0x4ba22b 0x7f61f04eb944 0x58ebef 0x51ae13 0x5b41c5 0x58f49e 0x51837f 0x5b41c5 0x4ba899 0x7f61f04eb944 0x58ebef 0x51ae13 0x5b41c5 0x58f49e 0x51b221 0x5b41c5 0x604133 0x631049 0x58ea5d 0x51ae13 0x5b41c5 0x4ba899 0x51908c 0x5b41c5 0x58f49e\n",
            "112it [00:46,  1.51it/s]tcmalloc: large alloc 1367605248 bytes == 0x161bd6000 @  0x7f61fe42a001 0x7f61f04aa1af 0x7f61f0500c23 0x7f61f05a986d 0x7f61f05aa17f 0x7f61f05aa2d0 0x4ba22b 0x7f61f04eb944 0x58ebef 0x51ae13 0x5b41c5 0x58f49e 0x51837f 0x5b41c5 0x4ba899 0x7f61f04eb944 0x58ebef 0x51ae13 0x5b41c5 0x58f49e 0x51b221 0x5b41c5 0x604133 0x631049 0x58ea5d 0x51ae13 0x5b41c5 0x4ba899 0x51908c 0x5b41c5 0x58f49e\n",
            "123it [00:54,  1.15it/s]tcmalloc: large alloc 1546313728 bytes == 0xc15d2000 @  0x7f61fe42a001 0x7f61f04aa1af 0x7f61f0500c23 0x7f61f05a986d 0x7f61f05aa17f 0x7f61f05aa2d0 0x4ba22b 0x7f61f04eb944 0x58ebef 0x51ae13 0x5b41c5 0x58f49e 0x51837f 0x5b41c5 0x4ba899 0x7f61f04eb944 0x58ebef 0x51ae13 0x5b41c5 0x58f49e 0x51b221 0x5b41c5 0x604133 0x631049 0x58ea5d 0x51ae13 0x5b41c5 0x4ba899 0x51908c 0x5b41c5 0x58f49e\n",
            "137it [01:05,  1.25it/s]tcmalloc: large alloc 1720901632 bytes == 0x161bd6000 @  0x7f61fe42a001 0x7f61f04aa1af 0x7f61f0500c23 0x7f61f05a986d 0x7f61f05aa17f 0x7f61f05aa2d0 0x4ba22b 0x7f61f04eb944 0x58ebef 0x51ae13 0x5b41c5 0x58f49e 0x51837f 0x5b41c5 0x4ba899 0x7f61f04eb944 0x58ebef 0x51ae13 0x5b41c5 0x58f49e 0x51b221 0x5b41c5 0x604133 0x631049 0x58ea5d 0x51ae13 0x5b41c5 0x4ba899 0x51908c 0x5b41c5 0x58f49e\n",
            "155it [01:22,  1.10it/s]tcmalloc: large alloc 1945362432 bytes == 0x460e8000 @  0x7f61fe42a001 0x7f61f04aa1af 0x7f61f0500c23 0x7f61f05a986d 0x7f61f05aa17f 0x7f61f05aa2d0 0x4ba22b 0x7f61f04eb944 0x58ebef 0x51ae13 0x5b41c5 0x58f49e 0x51837f 0x5b41c5 0x4ba899 0x7f61f04eb944 0x58ebef 0x51ae13 0x5b41c5 0x58f49e 0x51b221 0x5b41c5 0x604133 0x631049 0x58ea5d 0x51ae13 0x5b41c5 0x4ba899 0x51908c 0x5b41c5 0x58f49e\n",
            "172it [01:39,  1.72it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 1min 30s, sys: 9.1 s, total: 1min 39s\n",
            "Wall time: 1min 39s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(s.shape)\n",
        "print(emb.shape)\n",
        "s_df = tf.data.Dataset.from_tensor_slices(s)\n",
        "emb_df = tf.data.Dataset.from_tensor_slices(emb)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVBxb4NHwY-i",
        "outputId": "ca0a460c-6e09-4265-8ca2-2919915bab98"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(22000,)\n",
            "(22000, 512)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_recommenders as tfrs\n",
        "\n",
        "metrics = tfrs.metrics.FactorizedTopK(\n",
        "  candidates=emb_df.batch(128)\n",
        ")\n",
        "task = tfrs.tasks.Retrieval(\n",
        "  metrics=metrics\n",
        ")\n",
        "\n",
        "\n",
        "class QueryModel(tfrs.Model):\n",
        "  def __init__(self, embedding_model, random_projection_matrix = None, task = None) -> None:\n",
        "    super(QueryModel, self).__init__()\n",
        "    self.embedding_model:tf.keras.Model = embedding_model\n",
        "    self.random_projection_matrix:tf.Tesnor or None = random_projection_matrix\n",
        "    self.task: tf.keras.layers.Layer or None = task\n",
        "\n",
        "  #@tf.function(input_signature=[tf.TensorSpec([], tf.string)])\n",
        "  def __call__(self, input):\n",
        "    x = self.embedding_model(input)\n",
        "    if self.random_projection_matrix is not None:\n",
        "      x = tf.matmul(x, self.random_projection_matrix)\n",
        "    return x"
      ],
      "metadata": {
        "id": "Fkpp1AFtEDbq"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#rpm = random_projection_matrix_gen(512, 64)"
      ],
      "metadata": {
        "id": "gFeiKhqRPXS8"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = QueryModel(embedding_model, None, task)"
      ],
      "metadata": {
        "id": "3HSRJREWOUoB"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "import tensorflow_recommenders as tfrs\n",
        "IndexNN = tfrs.layers.factorized_top_k.ScaNN(model)\n",
        "IndexNN.index_from_dataset(s_df.batch(256).map(lambda x:(x,model(x))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kFl0Xu-gOddq",
        "outputId": "1e5898ff-b3f1-41e9-ec8d-737c39f63d1c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7f6169412d40> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7f6169412d40>: no matching AST found among candidates:\n",
            "\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: AutoGraph could not transform <function <lambda> at 0x7f6169412d40> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7f6169412d40>: no matching AST found among candidates:\n",
            "\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "CPU times: user 19min 4s, sys: 12 s, total: 19min 16s\n",
            "Wall time: 10min 19s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow_recommenders.layers.factorized_top_k.ScaNN at 0x7f60d16eecd0>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "_, res = IndexNN(tf.constant(['Looking for a python developer experienced in django and flask']))"
      ],
      "metadata": {
        "id": "oJ8x5sqcWgYq"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wlFft1HlW9HH",
        "outputId": "8dc06253-45c1-42a5-a158-005a22c66d7e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 10), dtype=string, numpy=\n",
              "array([[b'Looking for Python programmer who will be doing python coding all dayPythonREST APILinuxAWS',\n",
              "        b'Python Developer  Location: Houston, TXDuration: Fulltime Description:Client is looking for a high quality Python developer with loads of experience building scalable web applications to join its platform team. This is a highly technical position requiring a deep understanding of both Python and Django as well as PL/pgSQL and RDBMS technology.Requirements for this position 4+ years working with Python and Django Expected strong knowledge of Python 3.3+ and the latest Django software Strong knowledge of PL/pgSQL, PostgreSQL Strong knowledge of the Django ORM Strong knowledge of Python data structures and data flows Strong knowledge of the Linux command line and best practices as a Linux server administrator Some asynchronous programming experience  not necessarily in Python. Any further query you may contact at amit@iconma.com, please make sure to write the Job ID number is the subject Amit KumarICONMA LLCToll Free: (866) 5575424E Mail:     amit@iconma.com',\n",
              "        b'Hi, Please find the requirement below and let me know about your interest you can reach me on 6097519594 OR Swetha@kanisol.com Title : Python Developer with DevopsLocation : Columbus, OH Duration : 6+ months Responsibilities:Development  experience on Python and Ruby ( Mandatory)Interacting with Atlassian tools REST clients using Python and RubyConverting Ruby scripts into PythonDevelop REST clients using PythonAutomating build tasks using Jenkins and PythonRequired to be proficient in Ant, Maven, Gradle , Shell and Per Scripting.Should be familiar with Code Management Tools like Stash, Git, Jira and Confluence. SCM Process regarding Build, Release and Source Code Management.Qualifications:Expertise on Python Ruby and Shell scripting.Experience on Jenkins & its plugins , Ant, Maven and GradleExperience with Atlassian tools like Stash\\\\Bitbucket, Jira, and confluenceStrong on Mac OS, Linux and Windows ',\n",
              "        b'Our client is looking for a Python Developer to help improve and enhance their REST API, which is the core of their product ecosystem. The Python developer will help drive API design decisions and will be responsible for the full life cycle of API development to support the needs of their web, mobile and thirdparty applications used by their customers.Required Skills8+ years of over all development experience35 years of extensive experience with Python programming (middleware and some front end)Extensive Django framework experienceORMs (Object Relational Mapping) REST APIs (Extensive experience)Databases (Relational a must, LDAP a nice to have) Memcache (or similar caching mechanism) Linux (Fedora, CentOS)Golang experience is a plusPrefers someone who has a strong JavaScript background',\n",
              "        b\"Job Title: Python EngineerLocation: Midtown, NYCJob Type: ContractDuration: Long TermContact Info: Matt O'Brien  mobrien@techlink.com  2017862415Python Engineer with some Cloud AWS and Puppet exp.Key technologies:  Nginx, Haproxy, Apache, wsgi git, gitflow Python web frameworks such as Flask, Django Unit testing using frameworks such as unittest Understanding of test automation tools such as Selenium APM tools such as NewRelicInfrastructure:  AWS: at least 3+ years Docker: 1+ years Thorough understanding of TCP, IP and HTTP Understanding of network security and various encryption standards Able to configure and tune web servers Thorough understanding of Linux and reasonable knowledge on the Linux Kernel asic understanding of puppet and SCM tools Development languages:  Python : at least 5 years experience Go : 1+ years JavasScript frameworks such as Bootstrap, Angularjs HTML5 and templating frameworks Basic understanding of Bash and scripting Must have an understanding of modular applications and OOP Boto SDK and other cloud tools such as Terraform Json, Yaml, and REST frameworks \",\n",
              "        b'We have an urgent oprning with our client: Python Developer (100 % onsite)Duration: 6+ months contractClient Location: New York City, NYInterview Process: Telephonic / Skype Required Skills (mandatory):* Excellent programing background in Python (at least 3 years of experience)* Ability to write clean / optimized code* Proficient in developing in Linux/Unix environment* Basic quantitative background ',\n",
              "        b'Only looking for candidates that are VERY HANDS ON WITH PYTHON.  If you are not an advanced Python technical programer, please do not apply....Built warehouse infrastructure and functional data marts for reporting using ETL, Unix scripting etc.  Develop shell scripts to automate variety of tasks  Strong Python Exp.  Strong Informatica Exp. Good understanding of Database concepts like Query Plans, Indexing, Partitioning etc. ',\n",
              "        b\"My client is a fast growing VC backed FinTech startup located in Manhattan looking for several Sr. Python developers with 3+ years of experience. This is primarily a backend opportunity, mostly new development that will be responsible for building out the platform in Python. They are currently working with Mongo but moving into Postgres on the Database side.  External API Integration and pluggin experience is a plus.   Experience with Angular and Django/Flask is a plus.   They also have a Sr. Python Data engineering role that requires significant experience with databases and architechting a system.Salary commensurate with experience and competitive benefits package.  They are looking in the 100k150k range If you're qualified and interested, please send a copy of your resume to nyjobs@cypressg.com for immediate consideration.  Local candidates only please.  \",\n",
              "        b\"My client is a fast growing VC backed FinTech startup located in Manhattan looking for several Sr. Python developers with 3+ years of experience. This is primarily a backend opportunity, mostly new development that will be responsible for building out the platform in Python. They are currently working with Mongo but moving into Postgres on the Database side.  External API Integration and pluggin experience is a plus.   Experience with Angular and Django/Flask is a plus.   They also have a Sr. Python Data engineering role that requires significant experience with databases and architechting a system.Salary commensurate with experience and competitive benefits package.  They are looking in the 100k150k range If you're qualified and interested, please send a copy of your resume to nyjobs@cypressg.com for immediate consideration.  Local candidates only please.  \",\n",
              "        b' In person interview required in Hasbrouck Heights NJ Requirement Title: Python DeveloperLocation: Hasbrouck Heights NJ  We need the following developers to work from the Hasbrouck Heights NJ Job DescriptionWe are looking for a Python Web Developer responsible for managing the interchange of data between the server and the users. Your primary focus will be the development of all serverside logic, ensuring high performance and responsiveness to requests from the frontend. You will also be responsible for integrating the frontend elements built by your coworkers into the application; therefore, a basic understanding of frontend technologies is necessary as well. Responsibilities Writing reusable, testable, and efficient code Design and implementation of lowlatency, highavailability, and performant applications Integration of userfacing elements developed by frontend developers with server side logic Implementation of security and data protection Skills And Qualifications  Experience: 5  10 years Expert in Python, with knowledge of at least one Python web framework Django, Celery. Familiarity with some ORM (Object Relational Mapper) libraries Able to integrate multiple data sources and databases into one system Understanding of the threading limitations of Python, and multiprocess architecture Basic understanding of frontend technologies, such as JavaScript, HTML5, and CSS3 Knowledge of user authentication and authorization between multiple systems, servers, and environments Understanding of fundamental design principles behind a scalable application Familiarity with eventdriven programming in Python Able to create database schema that represent and support business processes Strong unit test and debugging skills']],\n",
              "      dtype=object)>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "module_no_signatures_path = os.path.join('/content', 'recommendation_module')\n",
        "print('Saving model...')\n",
        "tf.saved_model.save(IndexNN, module_no_signatures_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4M-JKhmXXMpl",
        "outputId": "a2b37b10-800a-49f3-9216-6c7c85026ee8"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.QueryModel object at 0x7f60ea3d3b50>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow_recommenders.tasks.retrieval.Retrieval object at 0x7f60ea3d3f50>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow_recommenders.metrics.factorized_top_k.FactorizedTopK object at 0x7f60d1f6db10>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow_recommenders.layers.factorized_top_k.Streaming object at 0x7f60ea3d3f90>, because it is not built.\n",
            "WARNING:absl:Found untraced functions such as query_with_exclusions, query_with_exclusions while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "imported = tf.saved_model.load(module_no_signatures_path)"
      ],
      "metadata": {
        "id": "z7XsAk77kj5S"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imported(tf.constant(['Looking for a python developer experienced in django and flask']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gtajM1E3L5Ki",
        "outputId": "701398b1-a17e-4a53-9b34-beb7dbc6eb31"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
              " array([[0.60617656, 0.56061584, 0.50254804, 0.4984687 , 0.47240555,\n",
              "         0.46601087, 0.45382226, 0.449813  , 0.449813  , 0.4401013 ]],\n",
              "       dtype=float32)>,\n",
              " <tf.Tensor: shape=(1, 10), dtype=string, numpy=\n",
              " array([[b'Looking for Python programmer who will be doing python coding all dayPythonREST APILinuxAWS',\n",
              "         b'Python Developer  Location: Houston, TXDuration: Fulltime Description:Client is looking for a high quality Python developer with loads of experience building scalable web applications to join its platform team. This is a highly technical position requiring a deep understanding of both Python and Django as well as PL/pgSQL and RDBMS technology.Requirements for this position 4+ years working with Python and Django Expected strong knowledge of Python 3.3+ and the latest Django software Strong knowledge of PL/pgSQL, PostgreSQL Strong knowledge of the Django ORM Strong knowledge of Python data structures and data flows Strong knowledge of the Linux command line and best practices as a Linux server administrator Some asynchronous programming experience  not necessarily in Python. Any further query you may contact at amit@iconma.com, please make sure to write the Job ID number is the subject Amit KumarICONMA LLCToll Free: (866) 5575424E Mail:     amit@iconma.com',\n",
              "         b'Hi, Please find the requirement below and let me know about your interest you can reach me on 6097519594 OR Swetha@kanisol.com Title : Python Developer with DevopsLocation : Columbus, OH Duration : 6+ months Responsibilities:Development  experience on Python and Ruby ( Mandatory)Interacting with Atlassian tools REST clients using Python and RubyConverting Ruby scripts into PythonDevelop REST clients using PythonAutomating build tasks using Jenkins and PythonRequired to be proficient in Ant, Maven, Gradle , Shell and Per Scripting.Should be familiar with Code Management Tools like Stash, Git, Jira and Confluence. SCM Process regarding Build, Release and Source Code Management.Qualifications:Expertise on Python Ruby and Shell scripting.Experience on Jenkins & its plugins , Ant, Maven and GradleExperience with Atlassian tools like Stash\\\\Bitbucket, Jira, and confluenceStrong on Mac OS, Linux and Windows ',\n",
              "         b'Our client is looking for a Python Developer to help improve and enhance their REST API, which is the core of their product ecosystem. The Python developer will help drive API design decisions and will be responsible for the full life cycle of API development to support the needs of their web, mobile and thirdparty applications used by their customers.Required Skills8+ years of over all development experience35 years of extensive experience with Python programming (middleware and some front end)Extensive Django framework experienceORMs (Object Relational Mapping) REST APIs (Extensive experience)Databases (Relational a must, LDAP a nice to have) Memcache (or similar caching mechanism) Linux (Fedora, CentOS)Golang experience is a plusPrefers someone who has a strong JavaScript background',\n",
              "         b\"Job Title: Python EngineerLocation: Midtown, NYCJob Type: ContractDuration: Long TermContact Info: Matt O'Brien  mobrien@techlink.com  2017862415Python Engineer with some Cloud AWS and Puppet exp.Key technologies:  Nginx, Haproxy, Apache, wsgi git, gitflow Python web frameworks such as Flask, Django Unit testing using frameworks such as unittest Understanding of test automation tools such as Selenium APM tools such as NewRelicInfrastructure:  AWS: at least 3+ years Docker: 1+ years Thorough understanding of TCP, IP and HTTP Understanding of network security and various encryption standards Able to configure and tune web servers Thorough understanding of Linux and reasonable knowledge on the Linux Kernel asic understanding of puppet and SCM tools Development languages:  Python : at least 5 years experience Go : 1+ years JavasScript frameworks such as Bootstrap, Angularjs HTML5 and templating frameworks Basic understanding of Bash and scripting Must have an understanding of modular applications and OOP Boto SDK and other cloud tools such as Terraform Json, Yaml, and REST frameworks \",\n",
              "         b'We have an urgent oprning with our client: Python Developer (100 % onsite)Duration: 6+ months contractClient Location: New York City, NYInterview Process: Telephonic / Skype Required Skills (mandatory):* Excellent programing background in Python (at least 3 years of experience)* Ability to write clean / optimized code* Proficient in developing in Linux/Unix environment* Basic quantitative background ',\n",
              "         b'Only looking for candidates that are VERY HANDS ON WITH PYTHON.  If you are not an advanced Python technical programer, please do not apply....Built warehouse infrastructure and functional data marts for reporting using ETL, Unix scripting etc.  Develop shell scripts to automate variety of tasks  Strong Python Exp.  Strong Informatica Exp. Good understanding of Database concepts like Query Plans, Indexing, Partitioning etc. ',\n",
              "         b\"My client is a fast growing VC backed FinTech startup located in Manhattan looking for several Sr. Python developers with 3+ years of experience. This is primarily a backend opportunity, mostly new development that will be responsible for building out the platform in Python. They are currently working with Mongo but moving into Postgres on the Database side.  External API Integration and pluggin experience is a plus.   Experience with Angular and Django/Flask is a plus.   They also have a Sr. Python Data engineering role that requires significant experience with databases and architechting a system.Salary commensurate with experience and competitive benefits package.  They are looking in the 100k150k range If you're qualified and interested, please send a copy of your resume to nyjobs@cypressg.com for immediate consideration.  Local candidates only please.  \",\n",
              "         b\"My client is a fast growing VC backed FinTech startup located in Manhattan looking for several Sr. Python developers with 3+ years of experience. This is primarily a backend opportunity, mostly new development that will be responsible for building out the platform in Python. They are currently working with Mongo but moving into Postgres on the Database side.  External API Integration and pluggin experience is a plus.   Experience with Angular and Django/Flask is a plus.   They also have a Sr. Python Data engineering role that requires significant experience with databases and architechting a system.Salary commensurate with experience and competitive benefits package.  They are looking in the 100k150k range If you're qualified and interested, please send a copy of your resume to nyjobs@cypressg.com for immediate consideration.  Local candidates only please.  \",\n",
              "         b' In person interview required in Hasbrouck Heights NJ Requirement Title: Python DeveloperLocation: Hasbrouck Heights NJ  We need the following developers to work from the Hasbrouck Heights NJ Job DescriptionWe are looking for a Python Web Developer responsible for managing the interchange of data between the server and the users. Your primary focus will be the development of all serverside logic, ensuring high performance and responsiveness to requests from the frontend. You will also be responsible for integrating the frontend elements built by your coworkers into the application; therefore, a basic understanding of frontend technologies is necessary as well. Responsibilities Writing reusable, testable, and efficient code Design and implementation of lowlatency, highavailability, and performant applications Integration of userfacing elements developed by frontend developers with server side logic Implementation of security and data protection Skills And Qualifications  Experience: 5  10 years Expert in Python, with knowledge of at least one Python web framework Django, Celery. Familiarity with some ORM (Object Relational Mapper) libraries Able to integrate multiple data sources and databases into one system Understanding of the threading limitations of Python, and multiprocess architecture Basic understanding of frontend technologies, such as JavaScript, HTML5, and CSS3 Knowledge of user authentication and authorization between multiple systems, servers, and environments Understanding of fundamental design principles behind a scalable application Familiarity with eventdriven programming in Python Able to create database schema that represent and support business processes Strong unit test and debugging skills']],\n",
              "       dtype=object)>)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/model.zip /content/recomendations_module"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_Pt393nGEhu",
        "outputId": "5588982b-b564-4705-fcf6-ee2ed858e98c"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/recomendations_module/ (stored 0%)\n",
            "  adding: content/recomendations_module/variables/ (stored 0%)\n",
            "  adding: content/recomendations_module/variables/variables.data-00000-of-00001 (deflated 17%)\n",
            "  adding: content/recomendations_module/variables/variables.index (deflated 79%)\n",
            "  adding: content/recomendations_module/assets/ (stored 0%)\n",
            "  adding: content/recomendations_module/saved_model.pb (deflated 11%)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}